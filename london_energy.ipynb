{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Monaghan/LondonEnergy/blob/main/london_energy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRs25nraMn2Y"
      },
      "source": [
        "#London Energy Consumption Prediction\n",
        "###Project Overview\n",
        "This project tackles the challenge of predicting daily total electrical consumption for customers within various London boroughs. Developed as a data science consulting engagement for a fictional London energy company, this solution provides crucial insights for operational planning, resource allocation, and strategic decision-making.\n",
        "\n",
        "By integrating disparate datasets – historical London weather data and London home energy consumption – we've built a predictive model that can forecast energy demand, empowering the energy company to anticipate consumption patterns and optimize their services.\n",
        "\n",
        "###Problem Statement\n",
        "London's dynamic energy landscape necessitates accurate forecasting of electrical consumption to ensure grid stability, optimize energy procurement, and enhance customer satisfaction. Without a robust predictive model, energy companies face challenges like inefficient resource allocation, potential supply shortages, and missed opportunities for demand-side management.\n",
        "\n",
        "###Project Goals\n",
        "Data Integration: Combine and preprocess diverse datasets (weather and energy consumption) to create a unified, analyzable dataset.\n",
        "\n",
        "###Exploratory Data Analysis (EDA):\n",
        " Uncover key trends, patterns, and correlations within the combined data, particularly focusing on the relationship between weather variables and energy consumption.\n",
        "\n",
        "###Predictive Model Development:\n",
        "Engineer features and develop a machine learning model capable of accurately predicting daily total electrical consumption per borough.\n",
        "\n",
        "###Model Evaluation:\n",
        "Rigorously assess the model's performance using appropriate metrics and techniques.\n",
        "\n",
        "###Actionable Insights:\n",
        "Translate complex data analysis and model predictions into clear, concise, and actionable insights for a non-technical executive audience (the fictional CEO).\n",
        "\n",
        "###Communication Strategy:\n",
        "Develop and present a compelling narrative that highlights the business value and implications of the project findings.\n",
        "\n",
        "##Data Sources\n",
        "This project utilizes two primary datasets, adapted and transformed for the purpose of this analysis:\n",
        "\n",
        "London Daily Weather (1979-2021): Provides historical weather conditions including temperature, precipitation, and other relevant meteorological factors.\n",
        "\n",
        "Original Source: Kaggle: London Daily Weather 1979 to 2021\n",
        "\n",
        "London Hourly Energy Dataset (2011-2014): Contains hourly energy consumption data for various London homes, including borough information.\n",
        "\n",
        "Original Source: Kaggle: London Hourly Energy Dataset 2011 to 2014\n",
        "\n",
        "Technical Stack\n",
        "Programming Language: Python\n",
        "\n",
        "###Key Libraries:\n",
        "\n",
        "pandas for data manipulation and analysis\n",
        "\n",
        "numpy for numerical operations\n",
        "\n",
        "scikit-learn for machine learning model development (e.g., regression models, preprocessing)\n",
        "\n",
        "matplotlib and seaborn for data visualization\n",
        "\n",
        "(Potentially) Jupyter Notebook for reproducible analysis and presentation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "London daily weather 1979 to 2021:\n",
        "\n",
        "* https://www.kaggle.com/datasets/emmanuelfwerr/london-weather-data\n",
        "\n",
        "\n",
        "\n",
        "London hourly energy dataset 2011 to 2014:\n",
        "\n",
        "* https://www.kaggle.com/datasets/emmanuelfwerr/london-homes-energy-data"
      ],
      "metadata": {
        "id": "vRpJbCVI9mdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uTbHOvIl9Mlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# London Weather Dataset"
      ],
      "metadata": {
        "id": "J8S-6RChYawU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Read the [London weather dataset](https://drive.google.com/file/d/1eT1YaXgNIjxFPjPfzpQHWLaWwK54s_uC/view?usp=sharing) into a Pandas Dataframe**"
      ],
      "metadata": {
        "id": "p-R9qJUv0tQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XaPpU66VXbhS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import Dataset - I'd downloaded and saved to google drive\n",
        "\n",
        "\n",
        "File_ID = 'FILE_ID'\n",
        "download_link = 'GOOGLEDRIVELINK'\n",
        "Full_link = download_link.replace('FILE_ID', File_ID)\n"
      ],
      "metadata": {
        "id": "ftYQBH6KNAYd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather = pd.read_csv(File_ID)"
      ],
      "metadata": {
        "id": "zzgFHgEWOEkz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "37ef54af-5f0e-42bc-ace7-4ffe33447fca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'FILE_ID'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-2519669196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_weather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFile_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FILE_ID'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical analysis"
      ],
      "metadata": {
        "id": "Z0FXfC84y_Zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore the London weather dataset stats"
      ],
      "metadata": {
        "id": "tf7oZzSQ02EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Initial Data Inspection ---\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "# This helps to quickly preview the data, column names, and initial data types.\n",
        "\n",
        "df_weather.head()\n"
      ],
      "metadata": {
        "id": "-yWgpXEIr_gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Feature Engineering: Date Transformation ---\n",
        "\n",
        "# Original 'date' column is likely in a numerical format (e.g., YYYYMMDD).\n",
        "# We extract year, month, and day into separate integer columns for easier analysis\n",
        "# (e.g., grouping by year, analyzing seasonal patterns by month, or daily trends).\n",
        "\n",
        "# Extract the first 4 characters for the year and convert to integer.\n",
        "df_weather['year'] = df_weather['date'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Extract characters from index 4 to 5 (6th character) for the month and convert to integer.\n",
        "df_weather['month'] = df_weather['date'].astype(str).str[4:6].astype(int)\n",
        "\n",
        "# Extract characters from index 6 onwards for the day and convert to integer.\n",
        "df_weather['day'] = df_weather['date'].astype(str).str[6:].astype(int)\n",
        "\n",
        "# --- (Commented Out Alternative Methods) ---\n",
        "# These lines show alternative ways that were likely explored for date extraction.\n",
        "# It's good practice to keep such exploratory code commented if not actively used,\n",
        "# as it can serve as a reference for future work or understanding past attempts.\n",
        "\n",
        "#df_weather['year'] = df_weather['date'].str[:5].astype(int)\n",
        "#df_weather['month'] = df_weather['date'].str[5:7].astype(int)\n",
        "#df_weather['day'] = df_weather['date'].str[7:].astype(int)\n",
        "#df_weather['month'] = df_weather['date'].apply(lambda x: int(x[6:8]))\n",
        "#df_weather['year'] = df_weather['date'].apply(lambda x: int(x[9:]))"
      ],
      "metadata": {
        "id": "pAvS9IhT2xEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Post-Transformation Inspection ---\n",
        "\n",
        "# Display the first 5 rows again to confirm the new 'year', 'month', 'day' columns\n",
        "# have been added correctly.\n",
        "df_weather.head()\n"
      ],
      "metadata": {
        "id": "tu5VbUs5Xtvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the last 5 rows of the DataFrame.\n",
        "# Useful for checking the end of the dataset, especially after transformations,\n",
        "# and to see the data range.\n",
        "\n",
        "df_weather.tail()\n"
      ],
      "metadata": {
        "id": "oAPOq_KAXtvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics for numerical columns.\n",
        "# This provides summary metrics like count, mean, standard deviation, min, max,\n",
        "# and quartiles, which are essential for understanding the distribution and\n",
        "# central tendency of your data.\n",
        "df_weather.describe()"
      ],
      "metadata": {
        "id": "ADGZn16oXtvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a concise summary of the DataFrame.\n",
        "# This includes the index dtype and column dtypes, non-null values, and memory usage.\n",
        "# It's vital for checking data types and identifying columns with missing values quickly.\n",
        "df_weather.info()"
      ],
      "metadata": {
        "id": "dalGrsohXtvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DBjdq1N6-gwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Quality and Exploration ---\n",
        "\n",
        "# Display all unique values in the 'cloud_cover' column.\n",
        "# This is useful for understanding the range of categories or discrete values\n",
        "# within a specific column. It can help identify inconsistencies or unexpected values.\n",
        "df_weather['cloud_cover'].unique()\n"
      ],
      "metadata": {
        "id": "u7QmBczmXtvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYOoBDyLXtvz"
      },
      "outputs": [],
      "source": [
        "# Calculate the pairwise correlation of all columns in the DataFrame.\n",
        "# Correlation matrices show the linear relationship between variables, ranging from -1 to 1.\n",
        "# - A value close to 1 indicates a strong positive linear relationship.\n",
        "# - A value close to -1 indicates a strong negative linear relationship.\n",
        "# - A value close to 0 indicates a weak or no linear relationship.\n",
        "# This helps in identifying potential features for modeling and understanding multicollinearity.\n",
        "df_weather.corr()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column and sum them up.\n",
        "# This provides a count of NaN (Not a Number) values for every column,\n",
        "# indicating data completeness and where imputation or handling of missing data might be needed.\n",
        "df_weather.isnull().sum()"
      ],
      "metadata": {
        "id": "_-kBP1-wXtvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaxLhov8Xtvz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Annual Weather Summary Aggregation ---\n",
        "\n",
        "# GroupBY the DataFrame by 'year' and calculate aggregate statistics for key weather metrics.\n",
        "# This helps to summarize weather patterns on an annual basis, revealing trends over time.\n",
        "\n",
        "\n",
        "result = df_weather.groupby('year')[['cloud_cover', 'global_radiation', 'sunshine', 'precipitation', 'mean_temp', 'max_temp', 'min_temp']].agg(\n",
        "    {\n",
        "        'global_radiation': ['min', 'max', 'mean'],\n",
        "        'cloud_cover': ['min', 'max', 'mean'],\n",
        "        'sunshine': ['min', 'max', 'mean'],\n",
        "        'precipitation': ['min', 'max', 'mean'],\n",
        "        'mean_temp': 'mean',\n",
        "        'max_temp': 'max',\n",
        "        'min_temp': 'min'\n",
        "    }\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisations\n"
      ],
      "metadata": {
        "id": "8-72i9HxX1nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np # Import numpy for select_dtypes\n"
      ],
      "metadata": {
        "id": "1I4iRxxW_sbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Exploratory Data Analysis: Visualizations ---\n",
        "\n",
        "# Create a scatter matrix for all numerical columns.\n",
        "# This plot provides pairwise scatter plots, histograms for each variable,\n",
        "# aiding in visualizing relationships and distributions quickly.\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(15, 15)) # Smaller for demonstration\n",
        "scatter_matrix(df_weather.select_dtypes(include=np.number), ax=ax) # Only numerical columns\n",
        "plt.suptitle('Scatter Matrix of Weather Data', y=1.02) # Add a suptitle\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TWP_kwWV7zzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a heatmap of the correlation matrix.\n",
        "# This visually represents the strength and direction of linear relationships\n",
        "# between numerical variables, with annotations showing correlation coefficients.\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "cm=df_weather.corr()\n",
        "sns.heatmap(cm,annot=True,cmap=\"Blues\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rUESKWq7_vJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot line graphs for the mean annual precipitation, sunshine, and mean temperature.\n",
        "# This visualizes trends of these key weather metrics over the years.\n",
        "\n",
        "plt.plot(result.index, result[('precipitation', 'mean')])\n",
        "plt.plot(result.index, result[('sunshine', 'mean')])\n",
        "plt.plot(result.index, result[('mean_temp', 'mean')])\n",
        "\n",
        "plt.xlabel(\"Year\") # Y-axis label should reflect the metrics shown, or be more general if scales differ\n",
        "plt.ylabel(\"Annual Mean Values\") # More general, or specify units if all are comparable (e.g., \"Units\")\n",
        "plt.title(\"Annual Trends: Mean Precipitation, Sunshine, and Temperature\") # Accurate title\n",
        "\n",
        "plt.legend() # Add a legend to differentiate the lines\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RwACtSKgvfPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (Previous Grouping and Flattening - already discussed in detail) ---\n",
        "# Group by year for aggregated statistics (as explained in previous responses).\n",
        "# Using a standard dictionary format for aggregations.\n",
        "\n",
        "# Group by year for aggregated statistics\n",
        "# Using a standard dictionary format for aggregations\n",
        "result = df_weather.groupby('year')[['cloud_cover', 'global_radiation', 'sunshine', 'precipitation', 'mean_temp', 'max_temp', 'min_temp']].agg(\n",
        "    {\n",
        "        'global_radiation': ['min', 'max', 'mean'],\n",
        "        'cloud_cover': ['min', 'max', 'mean'],\n",
        "        'sunshine': ['min', 'max', 'mean'],\n",
        "        'precipitation': ['min', 'max', 'mean'],\n",
        "        'mean_temp': 'mean',\n",
        "        'max_temp': 'max',\n",
        "        'min_temp': 'min'\n",
        "    }\n",
        ")\n",
        "\n",
        "# The column naming with MultiIndex should now work correctly\n",
        "result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
        "print(\"\\nAggregated statistics by year (first 5 rows):\")\n",
        "print(result.head())\n",
        "print(result.tail())"
      ],
      "metadata": {
        "id": "cWwxifzMFrII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Specific Variable Visualizations ---\n",
        "\n",
        "# Visualize the trend of mean cloud cover over years.\n",
        "# A line plot helps to identify changes and patterns in cloudiness across the dataset's time span.\n",
        "\n",
        "# Cloud Cover Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='year', y='cloud_cover', data=df_weather.groupby('year')['cloud_cover'].mean().reset_index())\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Mean Cloud Cover')\n",
        "plt.title('Mean Cloud Cover Over Years')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Visualize the distribution of 'cloud_cover' using a histogram.\n",
        "# This shows the frequency of different cloud cover values, providing insight into its typical range and variability.\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_weather['cloud_cover'], bins=10, kde=True)\n",
        "plt.xlabel('Cloud Cover')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Cloud Cover')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F1kB0HLbfarr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the trend of mean snow depth over years.\n",
        "# A line plot to observe annual variations and trends in snow depth.\n",
        "# Snow Depth Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='year', y='snow_depth', data=df_weather.groupby('year')['snow_depth'].mean().reset_index())\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Mean Snow Depth')\n",
        "plt.title('Mean Snow Depth Over Years')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Visualize the distribution of 'snow_depth' using a histogram.\n",
        "# Shows the frequency distribution of snow depth values.\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_weather['snow_depth'], bins=10, kde=True)\n",
        "plt.xlabel('snow_depth')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of snow_depth')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pgWCKCtE9pjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# --- Scatter Plots for Relationships Between Variables ---\n",
        "\n",
        "\n",
        "# @title global_radiation vs max_temp\n",
        "# Scatter plot of 'global_radiation' vs 'max_temp'.\n",
        "# Helps visualize if higher global radiation is associated with higher maximum temperatures.\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df_weather.plot(kind='scatter', x='global_radiation', y='max_temp', s=32, alpha=.2)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "cLvwshmEVEsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title cloud_cover vs month\n",
        "\n",
        "# Scatter plot of 'cloud_cover' vs 'month'.\n",
        "# Explores the relationship between cloud cover and the month, potentially showing seasonal patterns.\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df_weather.plot(kind='scatter', x='cloud_cover', y='month', s=32, alpha=0.01, )\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "id": "rCstCCGLByWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of 'cloud_cover' vs 'sunshine'.\n",
        "# Visualizes the inverse relationship expected between cloud cover and sunshine hours.\n",
        "\n",
        "df_weather.plot(kind='scatter', x='cloud_cover', y='sunshine', s=32, alpha=.05)\n"
      ],
      "metadata": {
        "id": "7ZMUUr5Ufts9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of 'cloud_cover' vs 'max_temp'.\n",
        "# Explores how cloud cover might correlate with maximum daily temperatures.\n",
        "\n",
        "df_weather.plot(kind='scatter', x='cloud_cover', y='max_temp', s=32, alpha=.03)\n"
      ],
      "metadata": {
        "id": "fwCnpTNR6iKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Scatter plot of 'sunshine' vs 'global_radiation'.\n",
        "# Visualizes the direct relationship between sunshine duration and global solar radiation.\n",
        "\n",
        "# @title sunshine vs global_radiation\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df_weather.plot(kind='scatter', x='sunshine', y='global_radiation', s=32, alpha=.05)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "tetZsjr8Ut0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of 'cloud_cover' vs 'precipitation'.\n",
        "# Explores the relationship between cloudiness and the amount of precipitation.\n",
        "\n",
        "df_weather.plot(x='cloud_cover', y='precipitation', kind='scatter')"
      ],
      "metadata": {
        "id": "acoITbIuuIyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sXFv1zoBh7DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKWx2llB4ug1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Data - Cloud Cover"
      ],
      "metadata": {
        "id": "a0fh4fOhYDeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Imputation:\n",
        "Addressing Missing cloud_cover Data\n",
        "A significant challenge identified was the missing data in the cloud_cover column, particularly before the 1990s. To preserve valuable historical information rather than discarding it, a deep learning classification model was employed for imputation. This approach allows for more accurate and contextually relevant predictions of the missing cloud_cover values by leveraging patterns from other weather variables.\n",
        "\n",
        "###Imputation Process:\n",
        "\n",
        "Data Segregation: The dataset was split into two: one for training (complete cloud_cover data) and one for imputation (missing cloud_cover).\n",
        "\n",
        "###Data Preparation:\n",
        "The training data was standardized, and features/labels were prepared as NumPy arrays, then split into training and testing sets.\n",
        "\n",
        "###Model Training:\n",
        "A deep learning classification model was trained on the clean, prepared weather data.\n",
        "\n",
        "###Model Evaluation:\n",
        "The model's performance was rigorously tested to ensure its predictive capability.\n",
        "\n",
        "###Data Filling:\n",
        "The trained model was then used to predict and fill the missing cloud_cover values in the original DataFrame, completing the dataset for subsequent analysis."
      ],
      "metadata": {
        "id": "qHEv9dmG2_kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Missing Value Check & Data Separation ---\n",
        "\n",
        "# Display the count of missing values for each column in the original DataFrame.\n",
        "# This confirms the extent of missing data before any manipulation.\n",
        "df_weather.isnull().sum()\n"
      ],
      "metadata": {
        "id": "WB_8AYu3nzHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1NTmthOm7z_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare NumPy Arrays for Deep Learning Model ---\n",
        "\n",
        "### Explore unique values and their frequencies in 'cloud_cover'.\n",
        "### This helps understand the distribution of the target variable for the classification model.\n"
      ],
      "metadata": {
        "id": "vEM_1c0F7kh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather['cloud_cover'].unique()\n",
        "df_weather['cloud_cover'].value_counts()\n",
        "\n"
      ],
      "metadata": {
        "id": "w6ZCpYC6MsH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore unique values and their frequencies in 'snow_depth'.\n",
        "# This is a pre-emptive check, as 'snow_depth' also has many missing values that will need handling.\n",
        "\n",
        "df_weather['snow_depth'].unique()\n",
        "df_weather['snow_depth'].value_counts()\n",
        "\n"
      ],
      "metadata": {
        "id": "cYkdhJRd9NKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two new DataFrames based on the presence/absence of 'cloud_cover' NaN values.\n",
        "# 'df_weather_cloud_cover_nan': Contains rows where 'cloud_cover' is missing (to be imputed).\n",
        "# 'df_weather_clean': Contains rows where 'cloud_cover' is not missing (to be used for training).\n",
        "\n",
        "df_weather_cloud_cover_nan = df_weather[df_weather['cloud_cover'].isna()]\n",
        "df_weather_clean = df_weather[df_weather['cloud_cover'].notna()]"
      ],
      "metadata": {
        "id": "xzidT6Dd07aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the tail of 'df_weather_clean'.\n",
        "# Verify that this DataFrame ends with complete 'cloud_cover' data (no NaNs at the end).\n",
        "\n",
        "df_weather_clean.tail()"
      ],
      "metadata": {
        "id": "ACbjaS3d2Qpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the tail of 'df_weather_cloud_cover_nan'.\n",
        "# Verify that this DataFrame correctly contains rows with missing 'cloud_cover'.\n",
        "\n",
        "df_weather_cloud_cover_nan.tail()"
      ],
      "metadata": {
        "id": "xysj25l62M9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-check unique values and counts in the 'cloud_cover' column of the 'clean' DataFrame.\n",
        "# Confirm that 'cloud_cover' column in this DataFrame has no NaN values and shows expected categories.\n",
        "\n",
        "df_weather_clean['cloud_cover'].unique()\n",
        "df_weather_clean['cloud_cover'].value_counts()\n",
        "\n"
      ],
      "metadata": {
        "id": "9LgV6RvBm0b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another check for unique values in 'cloud_cover' of the clean DataFrame. (Redundant, can be removed)\n",
        "\n",
        "df_weather_clean['cloud_cover'].unique()\n"
      ],
      "metadata": {
        "id": "0xdUhuOa2pvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Handle Missing 'snow_depth' (Imputation Strategy for another column) ---\n",
        "# Replace NaN values in 'snow_depth' column with 0 in the 'df_weather_clean' DataFrame.\n",
        "# The assumption is that NaN in 'snow_depth' indicates no snow, especially outside winter.\n",
        "\n",
        "df_weather_clean['snow_depth'] = df_weather_clean['snow_depth'].fillna(0)\n"
      ],
      "metadata": {
        "id": "voRlxmM52tF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify unique values in 'snow_depth' after filling NaNs.\n",
        "# Expect to see 0 among the unique values, and no NaNs.\n",
        "\n",
        "df_weather_clean['snow_depth'].unique()"
      ],
      "metadata": {
        "id": "RS6hltdh3XYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Final Missing Value Checks Before Model Data Prep ---\n",
        "\n",
        "# Display missing values for the original DataFrame (before any drops).\n",
        "# This is typically a reminder of the initial state of missing data.\n",
        "\n",
        "df_weather.isnull().sum()\n",
        "\n",
        "# Display missing values for the 'cleaned' DataFrame.\n",
        "# This check should confirm that 'cloud_cover' NaNs are gone, and other NaNs might still exist.\n",
        "\n",
        "df_weather_clean.isnull().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "OC9B2s_amzIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any remaining rows with *any* NaN values from 'df_weather_clean'.\n",
        "# This ensures that the dataset used for training the model is completely free of missing data.\n",
        "\n",
        "df_weather_clean = df_weather_clean.dropna()\n",
        "\n",
        "# Final check to confirm no missing values remain in the 'clean' DataFrame.\n",
        "\n",
        "df_weather_clean.isnull().sum()"
      ],
      "metadata": {
        "id": "4ZpQzzuT3rWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare Data for Deep Learning Model (Features & Labels) ---\n",
        "\n",
        "# Create NumPy array for features (X) by dropping the 'cloud_cover' column from 'df_weather_clean'.\n",
        "# The data type is set to float32, common for numerical deep learning inputs.\n",
        "\n",
        "x = df_weather_clean.drop('cloud_cover', axis=1).to_numpy(dtype='float32')\n",
        "\n",
        "# Create NumPy array for labels (y) using the 'cloud_cover' column from 'df_weather_clean'.\n",
        "# The data type is set to int, as 'cloud_cover' is treated as a classification target.\n",
        "\n",
        "y = df_weather_clean['cloud_cover'].to_numpy(dtype='int')"
      ],
      "metadata": {
        "id": "qZY1MmVbVgnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Split Data into Training and Testing Sets ---\n",
        "\n",
        "# Define the boundary for splitting the data (80% for training, 20% for testing).\n",
        "\n",
        "boundary = int(x.shape[0] * 0.8)\n",
        "\n",
        "# Split features and labels into training sets.\n",
        "x_train = x[:boundary]\n",
        "y_train = y[:boundary]\n",
        "\n",
        "# Split features and labels into testing sets.\n",
        "\n",
        "x_test = x[boundary:]\n",
        "y_test = y[boundary:]"
      ],
      "metadata": {
        "id": "aaVQZY_sD01B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Standardization ---\n",
        "\n",
        "# Calculate the mean of each feature from the training data.\n",
        "means = x_train.mean(axis=0)\n",
        "\n",
        "# Calculate the standard deviation of each feature from the training data.\n",
        "stds = x_train.std(axis=0)"
      ],
      "metadata": {
        "id": "vX826XTe0H1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the training features.\n",
        "\n",
        "# (X - mean) / std ensures that features have a mean of 0 and standard deviation of 1.\n",
        "# This is crucial for many deep learning models to converge faster and perform better.\n",
        "\n",
        "x_train = (x_train - means) / stds"
      ],
      "metadata": {
        "id": "QqqTMNGq0Nsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (Commented Out: Class Weight Calculation) ---\n",
        "# This block is commented out, but its purpose is to calculate class weights.\n",
        "# Class weights are used in classification tasks, especially with imbalanced datasets,\n",
        "# to give more importance to minority classes during model training, preventing the model\n",
        "# from being biased towards the majority class.\n",
        "\n",
        "\"\"\"from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Get unique classes and their counts\n",
        "unique_classes = np.unique(y_train)\n",
        "\n",
        "\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=unique_classes,\n",
        "    y=y_train # Your integer labels for the training data\n",
        ")\n",
        "\n",
        "# Convert the array to a dictionary, which model.fit() expects\n",
        "class_weights_dict = dict(zip(unique_classes, class_weights_array))\n",
        "\n",
        "print(\"Calculated Class Weights:\")\n",
        "print(class_weights_dict)\n",
        "# Example output: {0: 0.5, 1: 2.5, 2: 0.8, ...} (where 0.5 means a majority class, 2.5 a minority)\"\"\""
      ],
      "metadata": {
        "id": "zUYBUnxDHqL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloud Cover Model"
      ],
      "metadata": {
        "id": "tBhacaVeYKho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep learning classification model using Tensorflow/Keras. This will:**\n",
        "*   Use inverted bottlenecks\n",
        "*   Use residual connections\n",
        "*   Use gelu activations\n",
        "*   Use a softmax for output layer\n",
        "*   Use the sparse categorical crossentropy loss\n",
        "*   Use the adam optimizer"
      ],
      "metadata": {
        "id": "mPNTcH5u8TwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Training: Deep Learning for Cloud Cover Imputation ---\n"
      ],
      "metadata": {
        "id": "asDK6axcDf8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shapes of the prepared NumPy arrays.\n",
        "# This is a crucial sanity check to ensure data dimensions are as expected\n",
        "# before feeding them into the neural network.\n",
        "\n",
        "print(\"Shape of training features (x_train):\", x_train.shape) # Expected: (num_samples_train, num_features)\n",
        "print(\"Shape of training labels (y_train):\", y_train.shape)   # Expected: (num_samples_train,) for sparse categorical\n",
        "print(\"Shape of testing features (x_test):\", x_test.shape)     # Expected: (num_samples_test, num_features)\n",
        "print(\"Shape of testing labels (y_test):\", y_test.shape)       # Expected: (num_samples_test,)\n"
      ],
      "metadata": {
        "id": "RREoZJVjWrXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of input features (columns) for the neural network.\n",
        "# This should match the second dimension of x_train (number of features after dropping 'cloud_cover').\n",
        "# Here, it's explicitly set to 12, implying 12 features are used to predict cloud cover.\n",
        "# inputs match shape (column number)\n",
        "\n",
        "inputs = keras.layers.Input(shape=(12,))\n",
        "z = inputs\n"
      ],
      "metadata": {
        "id": "B1tyVG4WDxVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Neural Network Architecture Definition (Custom Block Design) ---\n",
        "\n",
        "# First Dense (fully connected) layer.\n",
        "# Transforms the input features into a 256-dimensional representation.\n",
        "\n",
        "z = keras.layers.Dense(256)(z)\n",
        "\n",
        "# Loop to create a sequence of custom \"residual\" blocks.\n",
        "# This architecture helps in building deeper networks by allowing information\n",
        "# to skip layers, mitigating the vanishing gradient problem.\n",
        "\n",
        "for i in range(4):\n",
        "  s = z\n",
        "\n",
        "  z = keras.layers.LayerNormalization()(z)\n",
        "\n",
        "  # First Dense layer within the block, expanding dimensions.\n",
        "  # The factor 6*256 suggests an expansion ratio within the block.\n",
        "\n",
        "  z = keras.layers.Dense(6*256)(z)\n",
        "  z = keras.activations.gelu(z)# GELU (Gaussian Error Linear Unit) activation function.\n",
        "                                  # A popular alternative to ReLU, often used in modern architectures.\n",
        "  # Second Dense layer within the block, projecting back to the original dimension (256).\n",
        "  z = keras.layers.Dense(256)(z)\n",
        "\n",
        "  # Residual connection: Add the input 's' to the output 'z' of the block.\n",
        "  # This allows the network to learn residual functions and improves information flow.\n",
        "  z = keras.layers.Add()([s,z])\n",
        "\n",
        "# Final Layer Normalization before the output layer.\n",
        "z = keras.layers.LayerNormalization()(z)\n",
        "# Output Dense layer with 10 units.\n",
        "# The number of units corresponds to the number of unique classes in 'cloud_cover'.\n",
        "z = keras.layers.Dense(10)(z)\n",
        "# Softmax activation function for the output layer.\n",
        "# This converts the raw outputs (logits) into probability distributions over the 10 classes,\n",
        "# suitable for multi-class classification where classes are mutually exclusive.\n",
        "outputs = keras.activations.softmax(z)"
      ],
      "metadata": {
        "id": "9OUMvyPPK2bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as cloud cover has 0,1,2,3,4,5,6,7,8,9 as unique values need to use 10 in final layer"
      ],
      "metadata": {
        "id": "6zG4ENlxEYM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Keras Model.\n",
        "# Defines the model by specifying its input and output layers.\n",
        "\n",
        "model = keras.Model(inputs=[inputs], outputs=[outputs])"
      ],
      "metadata": {
        "id": "rh0oLbXZEbMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# EarlyStopping callback\n",
        "# monitor='val_loss': Monitors the validation loss\n",
        "# patience=10: Waits for 10 epochs with no improvement in val_loss before stopping\n",
        "# mode='min': Stops when the monitored quantity (val_loss) stops decreasing\n",
        "# restore_best_weights=True: Restores the model weights from the epoch with the best monitored value\n",
        "\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=20,\n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1 # To see messages when training stops or weights are restored\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j75W21uOEnX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Compilation and Summary ---\n",
        "\n",
        "# Set the number of training epochs.\n",
        "epochs = 30\n",
        "\n",
        "# Calculate steps per epoch. This is the number of batches per epoch.\n",
        "# It's calculated by dividing the total number of training samples by the batch size (assuming 32 if not specified).\n",
        "# np.ceil ensures that even partial batches are accounted for.\n",
        "steps_per_epoch = np.ceil(np.shape(x_train)[0] / 32)\n",
        "print(\"Steps per epoch =\", steps_per_epoch)\n",
        "\n",
        "# Compile the model.\n",
        "# This configures the model for training by specifying:\n",
        "\n",
        "model.compile(\n",
        "  loss=keras.losses.SparseCategoricalCrossentropy(),# Loss function for integer labels (0-9) classification.\n",
        "                                                       # Suitable when labels are integers, not one-hot encoded.\n",
        "    # Optimizer for updating model weights.\n",
        "\n",
        "  optimizer=keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(\n",
        "      # Learning rate schedule: Cosine Decay.\n",
        "      # Learning rate starts at initial_learning_rate and\n",
        "      # decays following a cosine curve to 0 over decay_steps.\n",
        "  initial_learning_rate=0.01,\n",
        "  decay_steps=epochs*steps_per_epoch)),# Total steps over all epochs for decay.\n",
        "  metrics=[\"accuracy\"] # Metric to monitor during training and evaluation.\n",
        ")\n",
        ")\n"
      ],
      "metadata": {
        "id": "QM2tSncfEgRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary of the model's architecture.\n",
        "# Shows layer types, output shapes, and the number of trainable parameters.\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "mA-64a6mFByj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test),)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4mwYxZM2FGJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Training Execution ---\n",
        "\n",
        "# Train the deep learning model.\n",
        "# The `model.fit` method trains the model for a fixed number of epochs.\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, # Training features.\n",
        "    y_train, # Training labels.\n",
        "    epochs=epochs, # Number of full passes through the training dataset.\n",
        "    batch_size=64, # Number of samples per gradient update.\n",
        "    validation_split=0.2, # Fraction of the training data to be used as validation data.\n",
        "                          # This creates a validation set directly from x_train/y_train.\n",
        "    # callbacks=[early_stopping_callback] # If early stopping was uncommented, it would be added here.\n",
        ")"
      ],
      "metadata": {
        "id": "rRoHd9fmIOjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Prediction ---\n",
        "\n",
        "# Make predictions on the test set.\n",
        "# The model outputs probabilities for each class (10 values for each sample in x_test).\n",
        "\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "id": "jN6LnAL1Co7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predicted probabilities to class labels (integers 0-9).\n",
        "# np.argmax selects the index (class) with the highest predicted probability for each sample.\n",
        "\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n"
      ],
      "metadata": {
        "id": "tOPsZupZCwA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 15: Model Evaluation on Test Set ---\n",
        "\n",
        "# Evaluate the model's performance on the unseen test set.\n",
        "# Returns the loss value and metric values (e.g., accuracy) for the test data.\n",
        "\n",
        "history = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {history}\")\n",
        "print(f\"Test Accuracy: {history[1]}\")"
      ],
      "metadata": {
        "id": "7yjR_Y55Fa6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the performance of this deep learning model of both the training and test sets. This will include the use of:**\n",
        "*   Sklearn confusion matrix\n",
        "*   Sklearn classification report"
      ],
      "metadata": {
        "id": "YtT4nrm59_Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Classification Report and Confusion Matrix ---\n",
        "\n",
        "# Import the confusion_matrix function from scikit-learn.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Compute the confusion matrix.\n",
        "# It's a table showing the number of correct and incorrect predictions for each class,\n",
        "# essential for understanding where the model is performing well or struggling.\n",
        "# Pass y_test (true labels) and y_pred_labels (predicted integer labels)\n",
        "cm = confusion_matrix(y_test, y_pred_labels)\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "uymms3MDL3AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the classification_report function from scikit-learn.\n",
        "from sklearn.metrics import classification_report\n",
        "# Generate a text report showing the main classification metrics (precision, recall, f1-score, support)\n",
        "# for each class, and overall averages.\n",
        "print(classification_report(y_test, y_pred_labels))"
      ],
      "metadata": {
        "id": "K-5v74fLZ5nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Evaluation filling in missing cloud_cover data**\n",
        "\n"
      ],
      "metadata": {
        "id": "QqaO--Agytc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy of this model is not very high - 27%  \n",
        "As there are 10 classes to choose from this is slightly higher than random selecting but not hugely significant. The model could get better if we took inot consideration the distribution of the classes in the datatset.\n",
        "\n",
        "It is not overfit but only getting around 3/10 correct.\n",
        "\n",
        "A significant factor contributing to this low accuracy is likely the imbalanced distribution of classes within the dataset. Future improvements should focus on addressing this class imbalance to enable the model to learn more effectively from under-represented cloud cover categories.\n",
        "\n"
      ],
      "metadata": {
        "id": "hCbcAuW3ytc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replace  missing cloud_cover data in the original dataset using this classification model**"
      ],
      "metadata": {
        "id": "vshQrFxvE0hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for inference and replacing missing values\n",
        "x_pred = df_weather_cloud_cover_nan.drop(['cloud_cover',], axis=1).to_numpy(dtype='float32')\n",
        "print(x_pred.shape)"
      ],
      "metadata": {
        "id": "T8W3uwFO0h0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardise the input with values from the training set (means and stds)\n",
        "\n",
        "x_pred_standardized = (x_pred - means) / stds\n",
        "\n",
        "print(f\"Shape of x_pred_standardized: {x_pred_standardized.shape}\")\n"
      ],
      "metadata": {
        "id": "pIaVNhQL8WdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make predictions on the *standardized inference data*\n",
        "y_pred_probabilities = model.predict(x_pred_standardized)\n",
        "\n",
        "print(f\"Shape of y_pred_probabilities: {y_pred_probabilities.shape}\")\n"
      ],
      "metadata": {
        "id": "yGaEkysp8hQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert the predicted probabilities to class labels (0-9)\n",
        "predicted_labels = np.argmax(y_pred_probabilities, axis=1)\n",
        "print(f\"Shape of predicted_labels: {predicted_labels.shape}\")\n",
        "\n",
        "\n",
        "# Ensure the lengths match before imputation\n",
        "if len(predicted_labels) == len(df_weather_cloud_cover_nan):\n",
        "    # Replace the NaN values in the original df_weather with the model's predictions\n",
        "    df_weather.loc[df_weather['cloud_cover'].isna(), 'cloud_cover'] = predicted_labels\n",
        "\n",
        "df_weather.head()"
      ],
      "metadata": {
        "id": "LiAENP1w8iLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# London Energy Dataset"
      ],
      "metadata": {
        "id": "Nq6ETQ6tYPQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read the [London energy dataset](https://drive.google.com/file/d/1elYGf3VwdDuMhkGGzFA9STGax6sq3iLT/view?usp=sharing) into a Pandas Dataframe**"
      ],
      "metadata": {
        "id": "KRHb5_WDIrjH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRtIMR-GD6RD"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/datasets/emmanuelfwerr/london-homes-energy-data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "File_ID = 'Your ID'\n",
        "download_link = 'Googlelink'\n",
        "London_link = download_link.replace('FILE_ID', File_ID)\n"
      ],
      "metadata": {
        "id": "ggqAi92pYng_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_weather_london = pd.read_csv(London_link)"
      ],
      "metadata": {
        "id": "vYyu_GJ4Yng_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du3fP9OGYnhA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** London energy dataset stats**"
      ],
      "metadata": {
        "id": "qMmTPVlVKCFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather_london.head()\n"
      ],
      "metadata": {
        "id": "ECzl7r5cxPLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_weather_london.tail()"
      ],
      "metadata": {
        "id": "4NDP1qxlxPLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather_london.describe()"
      ],
      "metadata": {
        "id": "1fFLAc8wxPLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather_london.info()"
      ],
      "metadata": {
        "id": "4mvpxpRqxPLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather_london['Borough'].unique()\n"
      ],
      "metadata": {
        "id": "YFvOlPefxPLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather_london['MWH'].unique()\n"
      ],
      "metadata": {
        "id": "IW9gvKNG0Nho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather_london.isnull().sum()"
      ],
      "metadata": {
        "id": "4uVhAd2UxPLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**London weather dataset visualisations. **"
      ],
      "metadata": {
        "id": "v3w59Rl6xPLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_weather_london['MWH'],alpha=0.5)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "_wvleNhZxPLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# 'Date' column  converted to datetime\n",
        "df_weather_london['Date'] = pd.to_datetime(df_weather_london['Date'])\n",
        "\n",
        "sns.lineplot(x='Date', y='MWH', data=df_weather_london)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('MWH (Megawatt Hours)')\n",
        "plt.title('Total MWH Consumption Over Time')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dQwRCK0KELnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisations\n"
      ],
      "metadata": {
        "id": "Ni-_op0sxPLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(15, 15)) # Smaller for demonstration\n",
        "scatter_matrix(df_weather_london.select_dtypes(include=np.number), ax=ax) # Only numerical columns\n",
        "plt.suptitle('Scatter Matrix of Weather Data', y=1.02) # Add a suptitle\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ptGXN3rlxPLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qbICP5XbO-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change the weather datasets date to the format YYYY-MM-DD. This is in prepartion for joining both the weather and energy datasets by date.**"
      ],
      "metadata": {
        "id": "WF6jfjuifID-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HB9dYv4yF6UO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs3wUyg0EMq6"
      },
      "outputs": [],
      "source": [
        "# check format of df_weather - it is in correct format\n",
        "df_weather.head()\n",
        "# use same column name\n",
        "df_weather = df_weather.rename(columns={'date': 'Date'})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather_london.head()"
      ],
      "metadata": {
        "id": "Wlm5sXlHFdbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** inner join using Pandas merge on the weather and energy datasets together by the date/Date columns**"
      ],
      "metadata": {
        "id": "QFhUs-TIgB_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqq1IW8fHoEy"
      },
      "outputs": [],
      "source": [
        "# Merge the two dataframes on the date column using an 'inner join'\n",
        "df_summed = df_weather_london.merge(df_weather, on='Date', how='inner')\n",
        "df_summed.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "\n",
        "# Create a map centered around London\n",
        "m = folium.Map(location=[51.5074, -0.1278], zoom_start=10, tiles='OpenStreetMap')\n",
        "\n",
        "# Create a MarkerCluster to add multiple markers\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "norm = colors.Normalize(vmin=df_summed['MWH'].min(), vmax=df_summed['MWH'].max())\n",
        "\n",
        "# Add markers with colour proportional to MWH values\n",
        "for index, row in df_summed.iterrows():\n",
        "    color = plt.cm.jet(norm(row['MWH']))\n",
        "    color_hex = colors.rgb2hex(color)\n",
        "    folium.CircleMarker(location=[row['Latitude'], row['Longitude']],\n",
        "                        color=color_hex,\n",
        "                        fill_color=color_hex,\n",
        "                        fill_opacity=0.5).add_to(marker_cluster)\n",
        "\n",
        "# Display the map\n",
        "m"
      ],
      "metadata": {
        "id": "vQo657lCceb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** One hot encode the categorical borough column in prepartion for the deep learning model**"
      ],
      "metadata": {
        "id": "LvYNPDpEg_-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding of categorical columns - try with, without or with pd.factorize (https://pandas.pydata.org/docs/reference/api/pandas.factorize.html)\n",
        "df_summed = pd.get_dummies(\n",
        "    df_summed, columns=['Borough']\n",
        ")"
      ],
      "metadata": {
        "id": "0i9t6jashRjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert bool to int\n",
        "for col in df_summed.select_dtypes(include='bool').columns:\n",
        "    df_summed[col] = df_summed[col].astype(int)"
      ],
      "metadata": {
        "id": "ESE-CJWoHSvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summed.head()\n"
      ],
      "metadata": {
        "id": "uSQyK70ihv9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Create Numpy arrays in preparation for training and testing your deep learning model. These will need to be standardised and split into training and testing sets. The label will be the MWH.**"
      ],
      "metadata": {
        "id": "--Kdfglkh3uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create numpy arrays\n",
        "x = df_summed.drop('MWH', axis=1).to_numpy(dtype='float32')\n",
        "y = df_summed['MWH'].to_numpy(dtype='float32')"
      ],
      "metadata": {
        "id": "mFnuxUX3q6rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test sets\n",
        "boundary = int(x.shape[0]*0.8)\n",
        "x_train = x[:boundary]\n",
        "y_train = y[:boundary]\n",
        "x_test = x[boundary:]\n",
        "y_test = y[boundary:]"
      ],
      "metadata": {
        "id": "a_J5wZW0PVID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardise the input\n",
        "means = x_train.mean(axis=0)\n",
        "stds = x_train.std(axis=0)\n",
        "x_train = (x_train - means) / stds\n",
        "\n"
      ],
      "metadata": {
        "id": "Whb7j76bHpjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# And test set with values from training set\n",
        "x_test = (x_test - means) / stds\n"
      ],
      "metadata": {
        "id": "-4E7m8wDHpjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "xTXzaQ4aW40e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create and train a deep learning regression model using Tensorflow/Keras. This will:**\n",
        "*   Use inverted bottlenecks\n",
        "*   Use residual connections\n",
        "*   Use gelu activations\n",
        "*   Use a gelu activation for output layer\n",
        "*   Use the mean squared error loss\n",
        "*   Use the adam optimizer"
      ],
      "metadata": {
        "id": "Y5K2-GRWi8ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 54 inputs\n",
        "inputs = keras.layers.Input(shape=(54,))\n",
        "z = inputs\n",
        "\n",
        "# Projection layer from 54 to 192\n",
        "z = keras.layers.Dense(192)(z)\n",
        "\n",
        "for i in range(3):\n",
        "  # Shortcut connection\n",
        "  s = z\n",
        "\n",
        "  # Layer norm\n",
        "  z = keras.layers.LayerNormalization()(z)\n",
        "\n",
        "  # Expand this dimension up to 4*192 neurons\n",
        "  z = keras.layers.Dense(4*192)(z)\n",
        "  z = keras.activations.gelu(z)\n",
        "  z = keras.layers.Dense(192)(z)\n",
        "\n",
        "  # Add the shortcut connection with the output of the block\n",
        "  z = keras.layers.Add()([s,z])\n",
        "\n",
        "# Regression output for one value with gelu activation as price\n",
        "z = keras.layers.Dense(1)(z)\n",
        "outputs = keras.activations.gelu(z)"
      ],
      "metadata": {
        "id": "c48RyrzVrjhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Model(inputs=[inputs], outputs=[outputs])"
      ],
      "metadata": {
        "id": "BOWYH3V4rjhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "epochs = 20\n",
        "steps_per_epoch = np.ceil(np.shape(x_train)[0] / 32)\n",
        "print(steps_per_epoch)\n",
        "\n",
        "model.compile(\n",
        "  loss=keras.losses.MeanSquaredError(),\n",
        "  optimizer=keras.optimizers.Adam(learning_rate=keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.01, decay_steps=epochs*steps_per_epoch)),\n",
        "  metrics=[\"mse\", 'mape']\n",
        ")"
      ],
      "metadata": {
        "id": "u6j_WjmNrjhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "wQDTlv9UrjhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=epochs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "90nwuI_krjhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the performance of this deep learning model of both the training and test sets. This will include the use of:**\n",
        "*   Sklearn root mean squared error\n",
        "*   Sklearn mean absolute percentage error"
      ],
      "metadata": {
        "id": "aa_Nvc9di8i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = model.evaluate(x_train, y_train)[1]\n",
        "print(mse)"
      ],
      "metadata": {
        "id": "FWPKgVvArjhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converts the multi dimensional array into 1D\n",
        "y_train_pred = model.predict(x_train).flatten() # .flatten() to convert (N, 1) to (N,)\n",
        "y_test_pred = model.predict(x_test).flatten()"
      ],
      "metadata": {
        "id": "XTbHUvsDMhwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "# Formatted to 4 decimal places\n",
        "\n",
        "# RMSE for Training Set\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "print(f\"Training RMSE: {rmse_train:.4f}\")\n",
        "\n",
        "# RMSE for Test Set\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "print(f\"Test RMSE: {rmse_test:.4f}\")\n"
      ],
      "metadata": {
        "id": "TncdYe7nM5Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatted to 2 decimal places as a percentage\n",
        "# MAPE for Training Set\n",
        "\n",
        "mape_train = mean_absolute_percentage_error(y_train, y_train_pred) * 100\n",
        "print(f\"Training MAPE: {mape_train:.2f}%\") # Formatted to 2 decimal places as a percentage\n",
        "\n",
        "# MAPE for Test Set\n",
        "mape_test = mean_absolute_percentage_error(y_test, y_test_pred) * 100\n",
        "print(f\"Test MAPE: {mape_test:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Vxt0JEZRNCYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Evaluate using this model for energy use prediction. potential issues around bias?**"
      ],
      "metadata": {
        "id": "6ZBRObOOokl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias - yes a couple of potential issues.\n",
        "\n",
        "Sampling Bias - the collected dataset may not be representative of the population as a whole. Using non representative areas - skewed towards high or low income areas.\n",
        "Energy usage changes overtime - new tech can have an impact in reducing and increasing energy consuption. Socialtal changes drive fluctuations in energy usage, london olympics / war etc. Need to be careful that the timeframe that is sampled is representative of general usage.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "smuJV7MJovcz"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}